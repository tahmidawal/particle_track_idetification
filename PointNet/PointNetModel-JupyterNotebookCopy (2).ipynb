{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e73b43dc",
   "metadata": {},
   "source": [
    "Necessary imports for the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82a3e4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pylab as plt\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import f1_score\n",
    "from random import sample\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bebb6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for the model\n",
    "\n",
    "\n",
    "N = 90 # Number of points per event\n",
    "num_classes = 5 # Number of classes we're classifying into\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a21242c",
   "metadata": {},
   "source": [
    "Loads data from the .npy files and and returns numpy arrays for further analysis. \n",
    "This data contains, the position, time and charge indentities in each events. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e5279a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    deuteron = np.load('./3DsmallDataParticleChargeArrays/deuteron_array3D.npy', allow_pickle=True)\n",
    "    he3 = np.load('./3DsmallDataParticleChargeArrays/he3_array3D.npy', allow_pickle=True)\n",
    "    he4 = np.load('./3DsmallDataParticleChargeArrays/he4_array3D.npy', allow_pickle=True)\n",
    "    proton = np.load('./3DsmallDataParticleChargeArrays/proton_array3D.npy', allow_pickle=True)\n",
    "    triton = np.load('./3DsmallDataParticleChargeArrays/triton_array3D.npy', allow_pickle=True)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6146c864",
   "metadata": {},
   "source": [
    "This function takes in numpy arrays containing the data for position, time, charge and converts it to \n",
    "3D tensorflow datatypes consisting of the position data in 3D\n",
    "\n",
    "Variables: \n",
    "\n",
    "N - the threshold for number of points considered per event.\n",
    "num_classes = The number of classifications there will be in the data\n",
    "BATCH_SIZE = the size of the sub arrays in the training/test dataset that will contain the events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0ee43b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    # For each event for loops to append the z-dimensional data which is the time.\n",
    "    time_deuteron = []\n",
    "    for event in range(len(deuteron)):\n",
    "        time_deuteron.append(deuteron[event][0])\n",
    "   \n",
    "    time_he3 = []\n",
    "    for event in range(len(he3)):\n",
    "        time_he3.append(he3[event][0])\n",
    "  \n",
    "    time_he4 = []\n",
    "    for event in range(len(he4)):\n",
    "        time_he4.append(he4[event][0])\n",
    "  \n",
    "    time_proton = []\n",
    "    for event in range(len(proton)):\n",
    "        time_proton.append(proton[event][0])\n",
    "\n",
    "    time_triton = []\n",
    "    for event in range(len(triton)):\n",
    "        time_triton.append(triton[event][0])\n",
    "        \n",
    "   \n",
    "    #only data where there was a particle detection will be stored in these arrays as PointNet model only needs the points where \n",
    "    #a particle will be present\n",
    "    only_detected_events_deuteron = [] \n",
    "    only_detected_events_he3 = []\n",
    "    only_detected_events_he4 = []\n",
    "    only_detected_events_proton = []\n",
    "    only_detected_events_triton = []\n",
    "    \n",
    "    num_events = len(triton) #should be 1500\n",
    "    num_rows = len(triton[0][0]) #should be 108\n",
    "    num_cols = len(triton[0][0][0]) #should be 112\n",
    "    \n",
    "    # For each events only takes the points where the particle was present\n",
    "    for event in range(num_events):\n",
    "\n",
    "        single_event_only_detected_events_deuteron = []\n",
    "        single_event_only_detected_events_he3 = []\n",
    "        single_event_only_detected_events_he4 = []\n",
    "        single_event_only_detected_events_proton = []\n",
    "        single_event_only_detected_events_triton = []\n",
    "        \n",
    "        #we floor divide time by 3, because a 108 by 112 by 291 system would be disproportionate. floor divisor subject to change later\n",
    "        #290.84//3 = 96 <-- we use 97 as the upper bound of the z-coordinate\n",
    "        \n",
    "        for row in range(num_rows):\n",
    "            for col in range(num_cols):\n",
    "                if time_deuteron[event][row][col] != []:\n",
    "                    single_event_only_detected_events_deuteron.append([row, col, int(time_deuteron[event][row][col][0]//3)])\n",
    "                    \n",
    "                if time_he3[event][row][col] != []:\n",
    "                    single_event_only_detected_events_he3.append([row, col, int(time_he3[event][row][col][0]//3) ])\n",
    "                    \n",
    "                if time_he4[event][row][col] != []:\n",
    "                    single_event_only_detected_events_he4.append([row, col, int(time_he4[event][row][col][0]//3)])\n",
    "                    \n",
    "                if time_proton[event][row][col] != []:\n",
    "                    single_event_only_detected_events_proton.append([row, col, int(time_proton[event][row][col][0]//3)])\n",
    "                    \n",
    "                if time_triton[event][row][col] != []:\n",
    "                    single_event_only_detected_events_triton.append([row, col, int(time_triton[event][row][col][0]//3)])\n",
    "              \n",
    "\n",
    "        # There must only be a constant number of points in each of the events for the PointNet model to work successfully. Here, we randomly take N points from the events which have N or more than N points. \n",
    "        if len(single_event_only_detected_events_deuteron)>=N:\n",
    "            only_detected_events_deuteron.append(sample(single_event_only_detected_events_deuteron,N))\n",
    "        else:\n",
    "            X = N-len(single_event_only_detected_events_deuteron)\n",
    "            for i in np.arange(X):\n",
    "                single_event_only_detected_events_deuteron.append((0,0,0))\n",
    "            only_detected_events_deuteron.append(sample(single_event_only_detected_events_deuteron , N))\n",
    "        if len(single_event_only_detected_events_he3)>=N:\n",
    "            only_detected_events_he3.append(sample(single_event_only_detected_events_he3,N))\n",
    "        else:\n",
    "            X = N-len(single_event_only_detected_events_he3)\n",
    "            for i in np.arange(X):\n",
    "                single_event_only_detected_events_he3.append((0,0,0))\n",
    "            only_detected_events_he3.append(sample(single_event_only_detected_events_he3 , N))\n",
    "        if len(single_event_only_detected_events_he4)>=N:\n",
    "            only_detected_events_he4.append(sample(single_event_only_detected_events_he4,N))\n",
    "        else:\n",
    "            X = N-len(single_event_only_detected_events_he4)\n",
    "            for i in np.arange(X):\n",
    "                single_event_only_detected_events_he4.append((0,0,0))\n",
    "            only_detected_events_he4.append(sample(single_event_only_detected_events_he4 , N))\n",
    "        if len(single_event_only_detected_events_proton)>=N:\n",
    "            only_detected_events_proton.append(sample(single_event_only_detected_events_proton,N))\n",
    "        else:\n",
    "            X = N-len(single_event_only_detected_events_proton)\n",
    "            for i in np.arange(X):\n",
    "                single_event_only_detected_events_proton.append((0,0,0))\n",
    "            only_detected_events_proton.append(sample(single_event_only_detected_events_proton , N))\n",
    "        if len(single_event_only_detected_events_triton)>=N:\n",
    "            only_detected_events_triton.append(sample(single_event_only_detected_events_triton,N))\n",
    "        else:\n",
    "            X = N-len(single_event_only_detected_events_triton)\n",
    "            for i in np.arange(X):\n",
    "                single_event_only_detected_events_triton.append((0,0,0))\n",
    "            only_detected_events_triton.append(sample(single_event_only_detected_events_triton , N))\n",
    "\n",
    "    # Creating the train arrays and test arrays\n",
    "    train_labels = []\n",
    "    train_points = []\n",
    "\n",
    "    # For each event we're inputting a different label number \n",
    "    for i in range(len(only_detected_events_deuteron)):\n",
    "        train_points.append(only_detected_events_deuteron[i])\n",
    "        train_labels.append(0)\n",
    "    for i in range(len(only_detected_events_he3)):\n",
    "        train_points.append(only_detected_events_he3[i])\n",
    "        train_labels.append(1)\n",
    "    for i in range(len(only_detected_events_he4)):\n",
    "        train_points.append(only_detected_events_he4[i])\n",
    "        train_labels.append(2)\n",
    "    for i in range(len(only_detected_events_proton)):\n",
    "        train_points.append(only_detected_events_proton[i])\n",
    "        train_labels.append(3)\n",
    "    for i in range(len(only_detected_events_triton)):\n",
    "        train_points.append(only_detected_events_triton[i])\n",
    "        train_labels.append(4)\n",
    "\n",
    "    # Splitting the data\n",
    "    train_events_final, test_events, train_labels_final, test_labels = train_test_split(train_points, train_labels, test_size=0.2)\n",
    "    \n",
    "    # For use in the PN model it needs to be converted to tensor_slices\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_events_final, train_labels_final))\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((test_events, test_labels))\n",
    "    \n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a3cfff",
   "metadata": {},
   "source": [
    "Used to shuffle points within each events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176b7f07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c0ce81ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "869\n"
     ]
    }
   ],
   "source": [
    "print(len(only_detected_events_deuteron))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f084b36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def augment(points, label):\n",
    "    \n",
    "    # shuffle points\n",
    "    points = tf.random.shuffle(points)\n",
    "    return points, label\n",
    "\n",
    "# This shuffles the points within each events. \n",
    "    train_dataset = train_dataset.shuffle(len(train_events_final)).map(augment).batch(BATCH_SIZE)\n",
    "    test_dataset = test_dataset.shuffle(len(test_events)).map(augment).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3d335f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "data = test_dataset.take(len(test_dataset))\n",
    "points, labels = list(data)[len(test_dataset)-2]\n",
    "points = points\n",
    "labels = labels.numpy()\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ee6646",
   "metadata": {},
   "source": [
    "The functions below including \n",
    "\n",
    "1. conv_bn(x, filters)\n",
    "2. dense_bn(x, filters)\n",
    "3. OrthogonalRegularizer(keras.regularizers.Regularizer)\n",
    "4. tnet(inputs, num_features)\n",
    "5. build_point_cloud_model(NUM_POINTS, NUM_CLASSES)\n",
    "\n",
    "are used to build our PointNet Neural Network. To learn more or understand this model better please visit either or both \n",
    "of these resourses.\n",
    "https://keras.io/examples/vision/pointnet/\n",
    "https://www.youtube.com/watch?v=GGxpqfTvE8c&t=878s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c45a6546",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv_bn(x, filters):\n",
    "    x = layers.Conv1D(filters, kernel_size=1, padding=\"valid\")(x)\n",
    "    x = layers.BatchNormalization(momentum=0.0)(x)\n",
    "    return layers.Activation(\"relu\")(x)\n",
    "\n",
    "\n",
    "def dense_bn(x, filters):\n",
    "    x = layers.Dense(filters)(x)\n",
    "    x = layers.BatchNormalization(momentum=0.0)(x)\n",
    "    return layers.Activation(\"relu\")(x)\n",
    "\n",
    "class OrthogonalRegularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self, num_features, l2reg=0.001):\n",
    "        self.num_features = num_features\n",
    "        self.l2reg = l2reg\n",
    "        self.eye = tf.eye(num_features)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = tf.reshape(x, (-1, self.num_features, self.num_features))\n",
    "        xxt = tf.tensordot(x, x, axes=(2, 2))\n",
    "        xxt = tf.reshape(xxt, (-1, self.num_features, self.num_features))\n",
    "        return tf.reduce_sum(self.l2reg * tf.square(xxt - self.eye))\n",
    "    \n",
    "def tnet(inputs, num_features):\n",
    "\n",
    "    # Initalise bias as the indentity matrix\n",
    "    bias = keras.initializers.Constant(np.eye(num_features).flatten())\n",
    "    reg = OrthogonalRegularizer(num_features)\n",
    "\n",
    "    x = conv_bn(inputs, 32)\n",
    "    x = conv_bn(x, 64)\n",
    "    x = conv_bn(x, 512)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = dense_bn(x, 256)\n",
    "    x = dense_bn(x, 128)\n",
    "    x = layers.Dense(\n",
    "        num_features * num_features,\n",
    "        kernel_initializer=\"zeros\",\n",
    "        bias_initializer=bias,\n",
    "        activity_regularizer=reg,\n",
    "    )(x)\n",
    "    feat_T = layers.Reshape((num_features, num_features))(x)\n",
    "    # Apply affine transformation to input features\n",
    "    return layers.Dot(axes=(2, 1))([inputs, feat_T])\n",
    "\n",
    "def build_point_cloud_model(NUM_POINTS, NUM_CLASSES):\n",
    "    \n",
    "    inputs = keras.Input(shape=(NUM_POINTS, 3))\n",
    "\n",
    "    x = tnet(inputs, 3)\n",
    "    x = conv_bn(x, 32)\n",
    "    x = conv_bn(x, 32)\n",
    "    x = tnet(x, 32)\n",
    "    x = conv_bn(x, 32)\n",
    "    x = conv_bn(x, 64)\n",
    "    x = conv_bn(x, 512)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = dense_bn(x, 256)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = dense_bn(x, 128)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=outputs, name=\"pointnet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d524ecc",
   "metadata": {},
   "source": [
    "This function utilizes the train_dataset to train the model and \n",
    "uses test_dataset to plot the learning curve and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0456bbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plots the Learning curve and saves to the give directory\n",
    "\n",
    "def plot_learning_curve(history):\n",
    "    LEARNING_CURVE_DIR = './Learning-Curve-PointNet-Model'\n",
    "    PRED_FIGS_DIR = './Prediction-label-images'\n",
    "\n",
    "    if not os.path.exists(LEARNING_CURVE_DIR):\n",
    "        os.makedirs(LEARNING_CURVE_DIR)\n",
    "\n",
    "    #want to save figure with the data and time.\n",
    "    #change to log_scale or power_transform accordingly:\n",
    "    today_time = str(datetime.today())\n",
    "    file_name = \"log_scale_LC{}{}\".format(today_time,'.png')\n",
    "    #file_name = \"power_transform_LC{}\".format(today_time,'.png')\n",
    "\n",
    "    save_fig_path = os.path.join(LEARNING_CURVE_DIR, file_name)\n",
    "    plt.plot(history[\"loss\"], label=\"training loss\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"validation loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5df41515",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plots the confusion matrix and saves to the given directory\n",
    "\n",
    "def plot_confusion_matrix(label, preds, classes, cmap=plt.cm.Blues):\n",
    "    \n",
    "    \n",
    "    CONFUSION_MATRIX_DIR = './confusion-matrix-directory'\n",
    "    \n",
    "    if not os.path.exists(CONFUSION_MATRIX_DIR):\n",
    "        os.makedirs(CONFUSION_MATRIX_DIR)\n",
    "    \n",
    "    today_time = str(datetime.today())\n",
    "    file_name = \"confusion_matrix{}{}\".format(today_time,'.png')\n",
    "    \n",
    "    save_fig_path = os.path.join(CONFUSION_MATRIX_DIR, file_name)    \n",
    "    \n",
    "    cm = confusion_matrix(label, preds)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4, 4), dpi=100)\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title='Confusion Matrix',\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')\n",
    "    \n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                    ha='center', va='center',\n",
    "                    color='white' if cm[i, j] > thresh else 'black')\n",
    "    fig.tight_layout()\n",
    "\n",
    "    \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "abee9ba9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/DAVIDSON/taawal/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/DAVIDSON/taawal/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/DAVIDSON/taawal/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/DAVIDSON/taawal/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/DAVIDSON/taawal/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/DAVIDSON/taawal/.local/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"pointnet\" is incompatible with the layer: expected shape=(None, 90, 3), found shape=(90, 3)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Compiles the model with the given learning rate. \u001b[39;00m\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m      7\u001b[0m loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m optimizer\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10e-6\u001b[39m),\n\u001b[1;32m      9\u001b[0m metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_categorical_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m result \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(train_dataset, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39mtest_dataset) \n\u001b[1;32m     13\u001b[0m plot_learning_curve(result\u001b[38;5;241m.\u001b[39mhistory)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#Converts the test dataset to numpy arrays for use in confusion matrix function\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m   1148\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/DAVIDSON/taawal/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/DAVIDSON/taawal/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/DAVIDSON/taawal/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/DAVIDSON/taawal/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/DAVIDSON/taawal/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/DAVIDSON/taawal/.local/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"pointnet\" is incompatible with the layer: expected shape=(None, 90, 3), found shape=(90, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    model = build_point_cloud_model(N, num_classes)\n",
    "    \n",
    "    classes = ['Deuteron',  'Helium-3',  'Helium-4', 'Proton',  'Triton']\n",
    "\n",
    "    # Compiles the model with the given learning rate. \n",
    "    model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=10e-6),\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    "    )\n",
    "\n",
    "    result = model.fit(train_dataset, epochs=1, validation_data=test_dataset) \n",
    "    plot_learning_curve(result.history)\n",
    "    \n",
    "    #Converts the test dataset to numpy arrays for use in confusion matrix function\n",
    "    all_labels = np.array([])\n",
    "    all_preds = np.array([])\n",
    "    \n",
    "    for i in range(len(test_dataset)):\n",
    "        data = test_dataset.take(i+1)\n",
    "        points, labels = list(data)[0]\n",
    "        points = points\n",
    "        labels = labels.numpy()\n",
    "\n",
    "        # run test data through model\n",
    "        preds = model.predict(points)\n",
    "        preds = tf.math.argmax(preds, -1)\n",
    "        preds = preds.numpy()\n",
    "        all_labels = np.array(all_labels.tolist()+labels.tolist())\n",
    "        all_preds = np.array(all_preds.tolist()+ preds.tolist())\n",
    "    \n",
    "    plot_confusion_matrix(all_labels, all_preds, classes, cmap=plt.cm.Blues)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f87ba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_dataset)):\n",
    "        data = test_dataset.take(i+1)\n",
    "        points, labels = list(data)[0]\n",
    "        points = points\n",
    "        labels = labels.numpy()\n",
    "\n",
    "        # run test data through model\n",
    "        preds = model.predict(points)\n",
    "        preds = tf.math.argmax(preds, -1)\n",
    "        preds = preds.numpy()\n",
    "        all_labels = np.array(all_labels.tolist()+labels.tolist())\n",
    "        all_preds = np.array(all_preds.tolist()+ preds.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ce9d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_dataset)):\n",
    "        data = test_dataset.take(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6296bf6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TakeDataset' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TakeDataset' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d659865",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
