{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "front-tourism",
   "metadata": {},
   "source": [
    "We will use Keras to solve a particle identification problem on simulated data. There are 5 possible outputs, so we are looking at a multi-class classification problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-briefs",
   "metadata": {},
   "source": [
    "Since we have a multi-class classification problem, each output should be the probability that a particle is x, for some x in {proton, deuteron, triton, he3, he4}. In effect we will turn this into 5 different binary classification problems. The way we do this is with one-hot encoding (I believe this is the right term for what follows). Instead of have a target vector with one column, we want our y samples (targets) to be arranged in a 5 column matrix, where each row will have a single one underneath the column that corresponds to the given output particle type. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-distance",
   "metadata": {},
   "source": [
    "First we upload the packages numpy , tensorflow , and pylab and load the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-simon",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pylab as plt\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-payday",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix_augmented(y_true,\n",
    "                          y_pred,\n",
    "                          classes,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"This function prints and plots the confusion matrix.\n",
    "    \n",
    "    Adapted from:\n",
    "    https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \n",
    "    Arguments:\n",
    "        y_true: Real class labels.\n",
    "        y_pred: Predicted class labels.\n",
    "        classes: List of class names.\n",
    "        title: Title for the plot.\n",
    "        cmap: Colormap to be used.\n",
    "    \n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        title = 'Confusion matrix'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4, 4), dpi=100)\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                    ha='center', va='center',\n",
    "                    color='white' if cm[i, j] > thresh else 'black')\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-indiana",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix_normalized(y_true,\n",
    "                          y_pred,\n",
    "                          classes,\n",
    "                          confusion,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"This function prints and plots the confusion matrix.\n",
    "    \n",
    "    Adapted from:\n",
    "    https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \n",
    "    Arguments:\n",
    "        y_true: Real class labels.\n",
    "        y_pred: Predicted class labels.\n",
    "        classes: List of class names.\n",
    "        title: Title for the plot.\n",
    "        cmap: Colormap to be used.\n",
    "    \n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        title = 'Confusion matrix'\n",
    "    \n",
    "    \n",
    "    cm = confusion\n",
    "    \n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4, 4), dpi=100)\n",
    "    #\n",
    "    im = ax.imshow( cm.astype(float) , interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], '.2f'),\n",
    "                    ha='center', va='center',\n",
    "                    color='white' if cm[i, j] > thresh else 'black')\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-discipline",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we map each particle name to a particle number.\n",
    "#We will never use the following line of code, but it is helpful to \n",
    "#know the convention this codes for mapping each particle to a number.\n",
    "mapping = {\"Proton\":0, \"Deuteron\":1, \"Triton\":2, \"He3\": 3, \"He4\":4}\n",
    "\n",
    "#We use a pandas dataframe bc it is nicer than numpy array.\n",
    "#parse -9999 as nan because it doesn't seem like real data:\n",
    "raw_data = pd.read_csv('Simulation.csv',na_values='-9999')\n",
    "\n",
    "#taking a look at the current state of the data, with nan values still there:\n",
    "print(raw_data.head)\n",
    "\n",
    "#the following comment demonstrates some of the pandas capabilty:\n",
    "#print(2*raw_data['Px'])\n",
    "\n",
    "#Here we create new columns in the dataframe:\n",
    "#For instance, the 'IsProton' column is true\n",
    "#when the corresponding row is a proton and false otherwise\n",
    "raw_data['IsProton']= raw_data['Type']=='Proton'\n",
    "raw_data['IsDeuteron']= raw_data['Type']=='Deuteron'\n",
    "raw_data['IsTriton']= raw_data['Type']=='Triton'\n",
    "raw_data['IsHe3']= raw_data['Type']=='He3'\n",
    "raw_data['IsHe4']= raw_data['Type']=='He4'\n",
    "\n",
    "#we now see how the above code has added five columns to the dataframe:\n",
    "print(raw_data.head)\n",
    "\n",
    "#We have not deleted any rows from the dataframe yet, so lets check its length before cutting:\n",
    "print('Pre-Clean Length of Data:', len(raw_data))\n",
    "\n",
    "#we specify in place because we want to make changes to the original copy of \n",
    "#raw_data itself, instead of storing this updata in a new object:\n",
    "#furthermore, dropna drops all rows in the data frame that \n",
    "#have at least one nan value \n",
    "raw_data.dropna(inplace=True)\n",
    "\n",
    "print('Check that each column has no nan values. If there') \n",
    "print('are no nan values left, the sum will be zero:',np.sum(raw_data.isna()))\n",
    "\n",
    "#we now expect the dataframe to be a degree shorter due to the cuts we made:\n",
    "print('Before cutting values x not in range -5000< x < 5000:',len(raw_data))\n",
    "\n",
    "#the final round of cuts we make will be cutting all data values\n",
    "#that are less than -5000 and greater than 5000. We do so in two lines of code per feature column:\n",
    "raw_data = raw_data[raw_data['dEdX'] < 5000]\n",
    "raw_data = raw_data[raw_data['dEdX'] > -5000]\n",
    "raw_data = raw_data[raw_data['Px'] < 5000]\n",
    "raw_data = raw_data[raw_data['Px'] > -5000]\n",
    "raw_data = raw_data[raw_data['Py'] < 5000]\n",
    "raw_data = raw_data[raw_data['Py'] > -5000]\n",
    "raw_data = raw_data[raw_data['Pz'] < 5000]\n",
    "raw_data = raw_data[raw_data['Pz'] > -5000]\n",
    "print('After cutting values x not in range -5000< x < 5000:', len(raw_data))\n",
    "\n",
    "#Let's see the statistics on our data now that we have made the desired edits:\n",
    "print(raw_data.describe())\n",
    "\n",
    "#Recall that we have true and false values under each column starting with\n",
    "#'IsProton'. We want to convert these to integer 0's and 1's.\n",
    "#We also want to make a new dataframe for this purpose, so that raw_data remains unchanged.\n",
    "#we use .copy() to ensure no data gets overwritten in raw_data, and .iloc allows\n",
    "#us to the index the columns by numbers instead of column headers. \n",
    "#raw_data_target is a copy of the columns beginning with \"IsProton\" in raw_data\n",
    "#raw_data_target = raw_data.iloc[:,7:12].copy()\n",
    "raw_data_target = raw_data.loc[:,['IsProton', 'IsDeuteron','IsTriton', 'IsHe3', 'IsHe4']].copy()\n",
    "\n",
    "#Now we turn these booleans into 1's and 0's:\n",
    "onehot_target_pandas = (1*raw_data_target)\n",
    "\n",
    "#We check to see if we have a dataframe with 1's and 0's:\n",
    "print(onehot_target_pandas.head())\n",
    "\n",
    "#We also need a target dataframe that is not one hot encoded, for the purposes\n",
    "#of the confusion matrix at the end. Essentially, the neural network will\n",
    "#create a distribution of predictions, and ...\n",
    "#target_pandas = raw_data.iloc[:,6].copy().replace({\"Proton\":0, \"Deuteron\":1, \"Triton\":2, \"He3\": 3, \"He4\":4})\n",
    "target_pandas = raw_data.loc[:,['Type']].copy().replace({\"Proton\":0, \"Deuteron\":1, \"Triton\":2, \"He3\": 3, \"He4\":4})\n",
    "\n",
    "print(target_pandas.head())\n",
    "#do a histogram. \n",
    "#log scale the column data. \n",
    "#add a number to each column. \n",
    "\n",
    "#finally, for use later on, lets deignate a dataframe that has just the \n",
    "#feature columns, dEdX, Px, Py, and Pz:\n",
    "#design_pandas = raw_data.iloc[:,0:4].copy()\n",
    "design_pandas = raw_data.loc[:,['dEdX','Px','Py','Pz']].copy()\n",
    "print(design_pandas.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-report",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#we want to see the dedX data in a histogram:\n",
    "plt.subplot(221)\n",
    "plt.hist(raw_data['Px'],log=True, bins=40)\n",
    "plt.subplot(222)\n",
    "plt.hist(raw_data['Py'],log=True, bins=40)\n",
    "plt.subplot(223)\n",
    "plt.hist(raw_data['Pz'],log=True, bins=40)\n",
    "plt.subplot(224)\n",
    "plt.hist(raw_data['dEdX'],log=True, bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-payment",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#we want to see the dedX data in a histogram:\n",
    "plt.subplot(221)\n",
    "plt.hist(np.log10(raw_data['Px']+5001),log=True, bins=40)\n",
    "plt.subplot(222)\n",
    "plt.hist(np.log10(raw_data['Py']+5001),log=True, bins=40)\n",
    "plt.subplot(223)\n",
    "plt.hist(np.log10(raw_data['Pz']+2),log=True, bins=40)\n",
    "plt.subplot(224)\n",
    "plt.hist(np.log10(raw_data['dEdX']+2),log=True, bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-drill",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(raw_data['Type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-blood",
   "metadata": {},
   "source": [
    "Now we create out design matrix, which we will call 'design'. Here, we want each row to contain an example, and each example to be described by it's dedx, px,py, and pz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-reynolds",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's convert the following pandas dataframes to numpy arrays:\n",
    "# 1 - target_pandas\n",
    "# 2 - onehot_target_pandas\n",
    "# 3 - design_pandas\n",
    "nponehot_target = onehot_target_pandas.to_numpy()\n",
    "\n",
    "design = design_pandas.to_numpy()\n",
    "\n",
    "#normalize the data using the minmaxscaler from sklearn. This is a class. \n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "design = scaler.fit_transform(design)\n",
    "#design = skl.preprocessing.normalize(design)\n",
    "\n",
    "target = target_pandas.to_numpy()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-order",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want to see the dedX data in a histogram:\n",
    "plt.subplot(221)\n",
    "plt.hist(design[:,0],log=True, bins=40)\n",
    "plt.subplot(222)\n",
    "plt.hist(design[:,1],log=True, bins=40)\n",
    "plt.subplot(223)\n",
    "plt.hist(design[:,2],log=True, bins=40)\n",
    "plt.subplot(224)\n",
    "plt.hist(design[:,3],log=True, bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-dinner",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(nponehot_target))\n",
    "print(len(nponehot_target)/5, \" examples for testing\")\n",
    "print(len(nponehot_target)- len(nponehot_target)/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data:\n",
    "TRAIN_SPLIT_INDEX = 532494\n",
    "\n",
    "y_train = nponehot_target[:TRAIN_SPLIT_INDEX]\n",
    "x_train = design[:TRAIN_SPLIT_INDEX]\n",
    "y_train_non_one_hot = target[:TRAIN_SPLIT_INDEX]\n",
    "\n",
    "#test data:\n",
    "x_test  = design[TRAIN_SPLIT_INDEX:]\n",
    "y_test  = target[TRAIN_SPLIT_INDEX:]\n",
    "\n",
    "print('Training Data:')\n",
    "print(x_train)\n",
    "print(y_train)\n",
    "\n",
    "print('Testing Data')\n",
    "print(x_test)\n",
    "print(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-sodium",
   "metadata": {},
   "source": [
    "Here is the code for plotting validation loss and training loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-actress",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(history):\n",
    "    plt.plot(history[\"loss\"], label=\"training loss\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"validation loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-berlin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the directory where model weights will be saved. Feel free to change it.\n",
    "CHECKPOINT_DIR = './model-checkpoints-PID/'\n",
    "\n",
    "if not os.path.exists(CHECKPOINT_DIR):\n",
    "    os.makedirs(CHECKPOINT_DIR)\n",
    "\n",
    "# The checkpoints will be saved with the corresponding epoch number in their filename\n",
    "ckpt_path = os.path.join(CHECKPOINT_DIR, 'weights.epoch.{epoch:02d}')\n",
    "\n",
    "# Setup checkpoint callback. We only save the weights, not the entire model\n",
    "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(ckpt_path, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-cedar",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\"loss\", \n",
    "                                                 factor=0.5, patience=5, min_lr = 0.00000001, mode=\"min\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-employer",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=10),\n",
    "    ckpt_callback,\n",
    "    reduce_lr\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-reliance",
   "metadata": {},
   "source": [
    "Let's use relu because we have a multi class logistic regression problem at hand. Let's start out with 3 hidden layers with 30 neurons, an output layer with 5 output options (because there were no Pions), and let's use categoricalcrossentropy because that seems fair enough (though we could switch to the log error from logistic regression videos later on). Also, let our validation split be 0.2, meaning we train on 800 examples and validate on 200. Let's start with a batch size of 64 and train for 300 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = tf.keras.Sequential() #Define the model object\n",
    "model.add(tf.keras.layers.Dense(30, input_shape=(4,), activation=\"relu\")) #Add the hidden layer\n",
    "model.add(tf.keras.layers.Dense(30, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(30, activation=\"relu\"))\n",
    "\n",
    "#output layer:\n",
    "model.add(tf.keras.layers.Dense(5, activation=\"softmax\"))\n",
    "\n",
    "\n",
    "model.compile(tf.keras.optimizers.Adam(lr=0.005),loss=tf.keras.losses.CategoricalCrossentropy()) #Adam optimizer and mean squared error loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-kitty",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = model.fit(x_train, y_train, epochs=300, batch_size=64, validation_split=0.2, callbacks=my_callbacks)\n",
    "\n",
    "plot_learning_curve(results.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-electronics",
   "metadata": {},
   "outputs": [],
   "source": [
    "EARLY_STOPPING_EPOCH = 193\n",
    "\n",
    "assert EARLY_STOPPING_EPOCH > 0, 'You need to set an early stopping point!'\n",
    "\n",
    "# Path the the checkpoint we want to load\n",
    "es_ckpt_path = os.path.join(CHECKPOINT_DIR, 'weights.epoch.{:02d}'.format(EARLY_STOPPING_EPOCH))\n",
    "\n",
    "# Load the weights from the desired checkpoint into the model\n",
    "model.load_weights(es_ckpt_path);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-choice",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)\n",
    "\n",
    "print(predictions[:5])\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "print(predictions[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-coverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix_1(y_test,predictions,normalize='true'):\n",
    "    confusion = confusion_matrix(y_test,predictions,normalize=normalize)\n",
    "    print('Recall')\n",
    "    print(confusion)\n",
    "print_confusion_matrix_1(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-immigration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix_2(y_test,predictions,normalize='pred'):\n",
    "    confusion = confusion_matrix(y_test,predictions,normalize=normalize)\n",
    "    print('Precision')\n",
    "    print(confusion)\n",
    "print_confusion_matrix_2(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imported-fleet",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix_augmented(y_test, predictions, [\"Protons\", \"Deuterons\", \"Triton\",\"He3\", \"He4\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-family",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize='pred'\n",
    "\n",
    "confusion = confusion_matrix(y_test, predictions, normalize=normalize)\n",
    "\n",
    "plot_confusion_matrix_normalized(y_test, predictions, [\"Protons\", \"Deuterons\", \"Triton\",\"He3\", \"He4\"], confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-synthesis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-health",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-ordinary",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
